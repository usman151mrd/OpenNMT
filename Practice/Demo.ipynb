{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# French to english Translation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[jai ete naif], target=[i was naive], predicted=[i was naive]\n",
      "src=[vous navez pas paye], target=[you didnt pay], predicted=[you didnt pay]\n",
      "src=[tom a lair triste], target=[tom sounds sad], predicted=[tom looks sad]\n",
      "src=[tout le monde se taisait], target=[all were silent], predicted=[all were silent]\n",
      "src=[je pourrais essayer], target=[i could try], predicted=[i could try]\n",
      "src=[allume le cierge], target=[light the candle], predicted=[light the candle]\n",
      "src=[tout le monde applaudit], target=[everyone cheered], predicted=[everyone cheered]\n",
      "src=[je hais les chiens], target=[i hate dogs], predicted=[i hate dogs]\n",
      "src=[je ten felicite], target=[congratulations], predicted=[congratulations]\n",
      "src=[estce en vente], target=[is that for sale], predicted=[is that for sale]\n",
      "BLEU-1: 91.34\n",
      "BLEU-2: 88.15\n",
      "BLEU-3: 81.20\n",
      "BLEU-4: 55.35\n",
      "test\n",
      "src=[le potage est froid], target=[the soup is cold], predicted=[the soups cold]\n",
      "src=[nous sommes enneiges], target=[were snowed in], predicted=[were adaptable]\n",
      "src=[tom est fidele], target=[tom is faithful], predicted=[tom is loyal]\n",
      "src=[quelquun a crie], target=[someone screamed], predicted=[everyone coughed]\n",
      "src=[tom adore les tulipes], target=[tom loves tulips], predicted=[tom loves tulips]\n",
      "src=[tom est interne], target=[tom is an intern], predicted=[tom is grim]\n",
      "src=[je deteste lhypocrisie], target=[i hate hypocrisy], predicted=[i hate sundays]\n",
      "src=[on a eu de la chance], target=[we got lucky], predicted=[it was too]\n",
      "src=[jai ete acquitte], target=[i was acquitted], predicted=[i was distracted]\n",
      "src=[ils se sont embrasses], target=[they kissed], predicted=[they embraced]\n",
      "BLEU-1: 57.63\n",
      "BLEU-2: 46.37\n",
      "BLEU-3: 39.45\n",
      "BLEU-4: 21.90\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "\treturn max(len(line.split()) for line in lines)\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad sequences with 0 values\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
    "\treturn X\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "\tprediction = model.predict(source, verbose=0)[0]\n",
    "\tintegers = [argmax(vector) for vector in prediction]\n",
    "\ttarget = list()\n",
    "\tfor i in integers:\n",
    "\t\tword = word_for_id(i, tokenizer)\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\ttarget.append(word)\n",
    "\treturn ' '.join(target)\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "\tactual, predicted = list(), list()\n",
    "\tfor i, source in enumerate(sources):\n",
    "\t\t# translate encoded source text\n",
    "\t\tsource = source.reshape((1, source.shape[0]))\n",
    "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
    "\t\traw_target, raw_src = raw_dataset[i]\n",
    "\t\tif i < 10:\n",
    "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "\t\tactual.append([raw_target.split()])\n",
    "\t\tpredicted.append(translation.split())\n",
    "\t# calculate BLEU score\n",
    "\tprint('BLEU-1: %.2f' % (100 * corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))))\n",
    "\tprint('BLEU-2: %.2f' % (100 * corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))))\n",
    "\tprint('BLEU-3: %.2f' % (100 * corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))))\n",
    "\tprint('BLEU-4: %.2f' % (100 * corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))))\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-french-both.pkl')\n",
    "train = load_clean_sentences('english-french-train.pkl')\n",
    "test = load_clean_sentences('english-french-test.pkl')\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "# prepare data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "\n",
    "# load model\n",
    "model = load_model('model_french.h5')\n",
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# German to English Translation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[tu ihm nichts], target=[dont hurt him], predicted=[dont hurt him]\n",
      "src=[das gibts doch nicht], target=[no way], predicted=[no is me]\n",
      "src=[schau genauer hin], target=[look closer], predicted=[look closer]\n",
      "src=[ich hasse sie nicht], target=[i dont hate you], predicted=[i dont hate you]\n",
      "src=[lass tom in ruhe], target=[leave tom alone], predicted=[leave tom alone]\n",
      "src=[mir geht es genauso], target=[i feel the same], predicted=[i feel the same]\n",
      "src=[ihr bezahlt], target=[youre paying], predicted=[youre paying]\n",
      "src=[sie waren bereit], target=[they were ready], predicted=[they were ready]\n",
      "src=[ich mag pizza], target=[i like pizza], predicted=[i like pizza]\n",
      "src=[tom setzte sich], target=[tom sat down], predicted=[tom sat down]\n",
      "BLEU-1: 86.45\n",
      "BLEU-2: 80.82\n",
      "BLEU-3: 72.84\n",
      "BLEU-4: 47.98\n",
      "test\n",
      "src=[wie gro bist du], target=[how tall are you], predicted=[how are you you]\n",
      "src=[er ist ein exknacki], target=[hes an excon], predicted=[he a a]\n",
      "src=[das ist wichtig], target=[its important], predicted=[its matters]\n",
      "src=[lugt nie], target=[never tell lies], predicted=[dont you lies]\n",
      "src=[ich mochte das aufessen], target=[i want to eat it], predicted=[i want to same]\n",
      "src=[ich bin spanischlehrer], target=[i teach spanish], predicted=[im am]\n",
      "src=[na gut auf gehts], target=[well lets go], predicted=[here over here]\n",
      "src=[mach dich fort], target=[go away], predicted=[get away]\n",
      "src=[ich hickse oft], target=[i often hiccup], predicted=[i often travel]\n",
      "src=[ich bin frei], target=[im free], predicted=[im awake]\n",
      "BLEU-1: 50.77\n",
      "BLEU-2: 37.78\n",
      "BLEU-3: 30.22\n",
      "BLEU-4: 15.27\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "\treturn max(len(line.split()) for line in lines)\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad sequences with 0 values\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
    "\treturn X\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "\tprediction = model.predict(source, verbose=0)[0]\n",
    "\tintegers = [argmax(vector) for vector in prediction]\n",
    "\ttarget = list()\n",
    "\tfor i in integers:\n",
    "\t\tword = word_for_id(i, tokenizer)\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\ttarget.append(word)\n",
    "\treturn ' '.join(target)\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "\tactual, predicted = list(), list()\n",
    "\tfor i, source in enumerate(sources):\n",
    "\t\t# translate encoded source text\n",
    "\t\tsource = source.reshape((1, source.shape[0]))\n",
    "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
    "\t\traw_target, raw_src = raw_dataset[i]\n",
    "\t\tif i < 10:\n",
    "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "\t\tactual.append([raw_target.split()])\n",
    "\t\tpredicted.append(translation.split())\n",
    "\t# calculate BLEU score\n",
    "\tprint('BLEU-1: %.2f' % (100 * corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))))\n",
    "\tprint('BLEU-2: %.2f' % (100 * corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))))\n",
    "\tprint('BLEU-3: %.2f' % (100 * corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))))\n",
    "\tprint('BLEU-4: %.2f' % (100 * corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))))\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "# prepare data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "\n",
    "# load model\n",
    "model = load_model('german_model.h5')\n",
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Danish to English Translation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[tom blev pakrt af en lastbil], target=[tom got hit by a truck], predicted=[tom was hit by a truck]\n",
      "src=[tom lo ikke], target=[tom didnt laugh], predicted=[tom didnt laugh]\n",
      "src=[vi har ingen penge], target=[we have no money], predicted=[we have no money]\n",
      "src=[jeg elsker at lse bger], target=[i love reading books], predicted=[i love read books books]\n",
      "src=[jeg hader interviews], target=[i hate interviews], predicted=[i hate interviews]\n",
      "src=[han kunne ikke lide at vre fattig], target=[he didnt like being poor], predicted=[he didnt like be poor]\n",
      "src=[tal aldrig med fremmede], target=[never talk to strangers], predicted=[never talk to strangers]\n",
      "src=[han fangede kyllingen], target=[he caught the chicken], predicted=[he caught the chicken]\n",
      "src=[vask skeerne], target=[wash the spoons], predicted=[wash the spoons]\n",
      "src=[jeg har ikke en tatovering], target=[i dont have a tattoo], predicted=[i dont have a tattoo]\n",
      "BLEU-1: 90.05\n",
      "BLEU-2: 85.06\n",
      "BLEU-3: 81.47\n",
      "BLEU-4: 71.47\n",
      "test\n",
      "src=[tom grinede], target=[tom was laughing], predicted=[tom nodded]\n",
      "src=[tom er sandsynligvis sulten], target=[tom is probably hungry], predicted=[tom is hungry]\n",
      "src=[lyt ikke til hende], target=[dont listen to her], predicted=[dont listen to to]\n",
      "src=[har du smurt dig ind i solcreme], target=[have you put on sunscreen], predicted=[did you you on sunscreen]\n",
      "src=[lyver du over for mig], target=[are you lying to me], predicted=[are you with to me]\n",
      "src=[det har vret en god dag], target=[its been a good day], predicted=[it looks very good day]\n",
      "src=[jeg tror det], target=[i think so], predicted=[i think it it]\n",
      "src=[det er ikke kun til pynt], target=[it isnt just for show], predicted=[its not not for for]\n",
      "src=[hvem ellers har du fortalt det til], target=[who else did you tell], predicted=[who did you found it]\n",
      "src=[det kan vente til i morgen], target=[it can wait until morning], predicted=[we be be]\n",
      "BLEU-1: 52.81\n",
      "BLEU-2: 38.87\n",
      "BLEU-3: 32.64\n",
      "BLEU-4: 20.83\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "\treturn max(len(line.split()) for line in lines)\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad sequences with 0 values\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
    "\treturn X\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "\tprediction = model.predict(source, verbose=0)[0]\n",
    "\tintegers = [argmax(vector) for vector in prediction]\n",
    "\ttarget = list()\n",
    "\tfor i in integers:\n",
    "\t\tword = word_for_id(i, tokenizer)\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\ttarget.append(word)\n",
    "\treturn ' '.join(target)\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "\tactual, predicted = list(), list()\n",
    "\tfor i, source in enumerate(sources):\n",
    "\t\t# translate encoded source text\n",
    "\t\tsource = source.reshape((1, source.shape[0]))\n",
    "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
    "\t\traw_target, raw_src = raw_dataset[i]\n",
    "\t\tif i < 10:\n",
    "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "\t\tactual.append([raw_target.split()])\n",
    "\t\tpredicted.append(translation.split())\n",
    "\t# calculate BLEU score\n",
    "\tprint('BLEU-1: %.2f' % (100 * corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))))\n",
    "\tprint('BLEU-2: %.2f' % (100 * corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))))\n",
    "\tprint('BLEU-3: %.2f' % (100 * corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))))\n",
    "\tprint('BLEU-4: %.2f' % (100 * corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))))\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-danish-both.pkl')\n",
    "train = load_clean_sentences('english-danish-train.pkl')\n",
    "test = load_clean_sentences('english-danish-test.pkl')\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "# prepare data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "\n",
    "# load model\n",
    "model = load_model('danish_model.h5')\n",
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python38564bite321ebe832964b3ea761822ebe375c96"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
